{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment1.ipynb","provenance":[{"file_id":"1S4OJG9mspEQIakt0AbEa2Ch4Rzqo3UaZ","timestamp":1618888217790}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"0-dk6OVYjOWi","executionInfo":{"status":"ok","timestamp":1618888303734,"user_tz":-120,"elapsed":1020,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy\n","import os\n","import csv \n","import math\n","from PIL import Image\n","from scipy import ndimage\n","\n","def load_dataset_2():\n","    with h5py.File('heart.h5', \"r\") as train_dataset:\n","        train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n","        train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n","\n","    with h5py.File('heart.h5', \"r\") as test_dataset:\n","        test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n","        test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n","        classes = np.array(test_dataset[\"list_classes\"][:])\n","\n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","\n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n","\n","def load_set():\n","    file = open(\"heart2.csv\")\n","    numpy_array = np.loadtxt(file, delimiter=\",\")\n","    #print (numpy_array.T)\n","    numpy_array_transpose = numpy_array.T \n","    m_train = math.ceil (0.8 * numpy_array_transpose.shape[1])\n","    #m_set = len(numpy_array_transpose) - m_train\n","    #print (\"mtrain is\",m_train)\n","    print(numpy_array_transpose.shape)\n","    train_set_x_orig = numpy_array_transpose[0:numpy_array_transpose.shape[0]-1,0:m_train]\n","    train_set_y_orig = numpy_array_transpose[numpy_array_transpose.shape[0]-1:numpy_array_transpose.shape[0],0:m_train]\n","    #train_set_y_orig = numpy_array_transpose[13:14,0:820]\n","    test_set_x_orig = numpy_array_transpose [0:numpy_array_transpose.shape[0]-1, m_train:numpy_array_transpose.shape[1]]\n","    test_set_y_orig = numpy_array_transpose[numpy_array_transpose.shape[0]-1 : numpy_array_transpose.shape[0],m_train:numpy_array_transpose.shape[1]]\n","    #print (train_set_x_orig)\n","    #print (\"here is y\" , len(train_set_y_orig))\n","    #print (train_set_y_orig)\n","    #print (\"here is test set\")\n","    #print (test_set_x_orig)\n","    #print (\"here is test y\", len(test_set_y_orig))\n","    #print (test_set_y_orig)\n","    #print(train_set_x_orig.shape)\n","    #print(train_set_y_orig.shape)\n","    #print(train_set_y_orig)\n","   #print(test_set_x_orig.shape)\n","    #print(test_set_y_orig.shape)\n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig\n","\n","\n","from scipy.special import expit\n","\n","def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    z -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(z)\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈ 1 line of code)\n","   # s = 1 / (1 + np.exp(-z))\n","    s = expit(z)\n","    ### END CODE HERE ###\n","    \n","    return s   "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDF2RrjCjeap","executionInfo":{"status":"ok","timestamp":1618888305922,"user_tz":-120,"elapsed":601,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}},"outputId":"27745e42-06c7-4bda-b3de-51aaf3734739"},"source":["os.getcwd()\n","load_set()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(14, 1025)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(array([[52., 53., 70., ..., 45., 42., 58.],\n","        [ 1.,  1.,  1., ...,  1.,  1.,  0.],\n","        [ 0.,  0.,  0., ...,  0.,  3.,  0.],\n","        ...,\n","        [ 2.,  0.,  0., ...,  2.,  2.,  1.],\n","        [ 2.,  0.,  0., ...,  0.,  2.,  2.],\n","        [ 3.,  3.,  3., ...,  2.,  2.,  1.]]),\n"," array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n","         1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n","         0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n","         0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n","         1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n","         1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n","         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n","         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n","         1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n","         1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n","         1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n","         1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n","         1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n","         1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n","         1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n","         0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n","         1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n","         0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n","         0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n","         1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n","         1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n","         0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n","         0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n","         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n","         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n","         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n","         1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n","         0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n","         1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n","         1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n","         0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n","         1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n","         1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n","         0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n","         1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n","         1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n","         0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n","         1., 1., 1., 0.]]),\n"," array([[61., 62., 60., ..., 47., 50., 54.],\n","        [ 1.,  0.,  1., ...,  1.,  0.,  1.],\n","        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n","        ...,\n","        [ 2.,  0.,  2., ...,  1.,  2.,  1.],\n","        [ 1.,  2.,  1., ...,  1.,  0.,  1.],\n","        [ 3.,  2.,  3., ...,  2.,  2.,  3.]]),\n"," array([[0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n","         1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n","         0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n","         0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n","         0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n","         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n","         1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n","         0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n","         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.]]))"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"vXPsqb4xpID2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618888308679,"user_tz":-120,"elapsed":683,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}},"outputId":"821acf3b-7d13-4077-c2ca-dc52b2bfe15d"},"source":["def initialize_with_zeros(dim):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n","    \n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","    \n","    Returns:\n","    w -- initialized vector of shape (dim, 1)\n","    b -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    w = np.zeros((dim, 1))\n","    b = 0\n","    ### END CODE HERE ###\n","\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b\n","\n","dim = 2\n","w, b = initialize_with_zeros(dim)\n","print (\"w = \" + str(w))\n","print (\"b = \" + str(b))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["w = [[0.]\n"," [0.]]\n","b = 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqECvSKLyqEU","executionInfo":{"status":"ok","timestamp":1618888311050,"user_tz":-120,"elapsed":940,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}},"outputId":"1553b79d-4934-46cb-e963-d338ee0c6068"},"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained above\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost for logistic regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","    \n","    Tips:\n","    - Write your code step by step for the propagation. np.log(), np.dot()\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    \n","    # FORWARD PROPAGATION (FROM X TO COST)\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","\n","    # compute activation \n","    A = sigmoid(np.dot(X.T,w) + b)          \n","    #print(A)\n","    cost = -(1/m) * np.sum(Y.T * np.log(A) + (1 - Y.T) * (np.log(1-A)) )                                 # compute cost\n","    ### END CODE HERE ###\n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    dw = (1/m) * np.dot(X,(A-Y.T))\n","\n","    db = (1/m) * np.sum(A-Y.T)\n","    \n","    ### END CODE HERE ###\n","   # print (dw.shape)\n","    #print (w.shape)\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost\n","\n","w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\n","#grads, cost = propagate(w, b, X, Y)\n","#print (\"dw = \" + str(grads[\"dw\"]))\n","#print (\"db = \" + str(grads[\"db\"]))\n","#print (\"cost = \" + str(cost))   \n","\n","train_set_x, train_set_y, test_set_x, test_set_y= load_set()\n","w, b = initialize_with_zeros(train_set_x.shape[0])\n","#rint (\"w is \", w )\n","#print (train_set_x.shape)\n","#print(np.dot(train_set_x.T,w))\n","grads, cost = propagate(w, b, train_set_x, train_set_y)\n","print(cost)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(14, 1025)\n","0.6931471805599453\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wq2AZHEPy85T","executionInfo":{"status":"ok","timestamp":1618888313981,"user_tz":-120,"elapsed":614,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}},"outputId":"1f884e6a-45ad-4bec-f64f-ee0746a8c200"},"source":["def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    print_cost -- True to print the loss every 100 steps\n","    \n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","    \n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        \n","        \n","        # Cost and gradient calculation (≈ 1-4 lines of code)\n","        ### START CODE HERE ### \n","        grads, cost = propagate(w, b, X, Y)\n","        ### END CODE HERE ###\n","        \n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # update rule (≈ 2 lines of code)\n","        ### START CODE HERE ###\n","        w = w - learning_rate * dw\n","        b = b - learning_rate * db\n","        ### END CODE HERE ###\n","        \n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","        \n","        # Print the cost every 100 training examples\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs\n","\n","train_set_x, train_set_y, test_set_x, test_set_y= load_set()\n","w, b = initialize_with_zeros(train_set_x.shape[0])\n","params, grads, costs = optimize(w, b, train_set_x, train_set_y, num_iterations= 200, learning_rate = 0.009, print_cost = 1)\n","\n","print (\"w = \" + str(params[\"w\"]))\n","print (\"b = \" + str(params[\"b\"]))\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(14, 1025)\n","Cost after iteration 0: 0.693147\n","Cost after iteration 100: nan\n","w = [[-1.29295534]\n"," [-0.12274005]\n"," [ 0.35982242]\n"," [-2.46184091]\n"," [-1.72467974]\n"," [-0.01389658]\n"," [ 0.04531177]\n"," [ 4.44546661]\n"," [-0.16413032]\n"," [-0.41018506]\n"," [ 0.13857896]\n"," [-0.33941075]\n"," [-0.1906794 ]]\n","b = -0.0074245814519839\n","dw = [[2.15335096e+01]\n"," [3.22999531e-01]\n"," [1.95619352e-01]\n"," [5.08426895e+01]\n"," [9.62393588e+01]\n"," [6.58536584e-02]\n"," [1.58764746e-01]\n"," [5.64655158e+01]\n"," [2.13243434e-01]\n"," [6.17784289e-01]\n"," [4.77020504e-01]\n"," [4.31611976e-01]\n"," [9.82676600e-01]]\n","db = 0.38576016799938095\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"W1PoKA81zUey","executionInfo":{"status":"error","timestamp":1618888318289,"user_tz":-120,"elapsed":639,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}},"outputId":"3bcfda94-8e1e-4fe5-888f-13ed9deadcdb"},"source":["\n","def predict(w, b, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    \n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","    \n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    \n","    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    A = sigmoid(np.dot(X.T,w) + b)\n","    ### END CODE HERE ###\n","    print(range(A.shape[1]))\n","    for i in range(A.shape[0]):        \n","        # Convert probabilities A[0,i] to actual predictions p[0,i]\n","        ### START CODE HERE ### (≈ 4 lines of code)\n","        #pass\n","        if(A[i,0] > 0.5):\n","            Y_prediction[0,i] = 1\n","        else:\n","            Y_prediction[0,i] = 0\n","        ### END CODE HERE ###\n","        \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction\n","\n","print (\"predictions = \" + str(predict(w, b, X)))\n","print(\"X is \" + str(X.shape))"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-2577c0544159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mY_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-2577c0544159>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(w, b, X)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mY_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Compute vector \"A\" predicting the probabilities of a cat being present in the picture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 13 into shape (2,1)"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00EoPiGyzfQL","executionInfo":{"status":"ok","timestamp":1618889572753,"user_tz":-120,"elapsed":1245680,"user":{"displayName":"omar samy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCmfDqcdEQCV53CC3tINvsmov2WAuwK0c4E5l_=s64","userId":"13338322223499296595"}},"outputId":"282dc621-30c7-47b9-db29-3800037b33af"},"source":["def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","    \"\"\"\n","    Builds the logistic regression model by calling the function you've implemented previously\n","    \n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n","    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n","    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n","    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n","    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","    print_cost -- Set to true to print the cost every 100 iterations\n","    \n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    \n","    # initialize parameters with zeros (≈ 1 line of code)\n","\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","    print(w.shape)\n","    # Gradient descent (≈ 1 line of code)\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n","    \n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Predict test/train set examples (≈ 2 lines of code)\n","    Y_prediction_test = predict(w,b, X_test)\n","    Y_prediction_train = predict(w,b, X_train)\n","\n","    ### END CODE HERE ###\n","\n","    # Print train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","\n","    \n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test, \n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","    \n","    return d\n","\n","train_set_x, train_set_y, test_set_x, test_set_y= load_set()\n","d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 5000000, learning_rate = 0.00001, print_cost = False)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(14, 1025)\n","(13, 1)\n","range(0, 1)\n","range(0, 1)\n","train accuracy: 86.46341463414635 %\n","test accuracy: 79.51219512195122 %\n"],"name":"stdout"}]}]}